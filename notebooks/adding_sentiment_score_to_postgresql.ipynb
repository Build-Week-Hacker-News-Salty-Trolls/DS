{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "cwd = os.getcwd()\n",
    "sys.path[0] = cwd[:cwd.rfind('/')]\n",
    "from data import postgres_helper as pgh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- Pandas settings --------------- #\n",
    "# Removes rows and columns truncation of '...'\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Connection to Google Cloud BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_create(client, ds_ref, table_name, count=30000):\n",
    "    \"\"\"\n",
    "    Create a pandas dataframe from Google bigquery connection\n",
    "    \n",
    "    Parameters\n",
    "    ---------------------------------------------------------\n",
    "    client:       bigquery connection\n",
    "    ds_ref:       a connected bigquery dataset reference\n",
    "    table_name:   (str) name of the table\n",
    "    count:        (int) the number of rows from the table to return\n",
    "    \n",
    "    Output\n",
    "    ---------------------------------------------------------\n",
    "    Returns a pandas dataframe\n",
    "    \"\"\"\n",
    "    table_ref = ds_ref.table(table_name)\n",
    "    table = client.get_table(table_ref)\n",
    "    \n",
    "    df = client.list_rows(table, max_results=count).to_dataframe()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Cloud credentials\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '../saltyhackers-bigquery.json'\n",
    "\n",
    "# Open bigquery client connection\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Create bigquery dataset reference\n",
    "hn_ref = client.dataset('hacker_news', project='bigquery-public-data')\n",
    "\n",
    "# Get 'comments' table from bigquery\n",
    "# Create dataframe with 50000 rows\n",
    "# ElephantSQL limit it 20MB\n",
    "\n",
    "comm_df = df_create(client, hn_ref, 'comments', 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sneak Peek at dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>by</th>\n",
       "      <th>author</th>\n",
       "      <th>time</th>\n",
       "      <th>time_ts</th>\n",
       "      <th>text</th>\n",
       "      <th>parent</th>\n",
       "      <th>deleted</th>\n",
       "      <th>dead</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2701393</td>\n",
       "      <td>5l</td>\n",
       "      <td>5l</td>\n",
       "      <td>1309184881</td>\n",
       "      <td>2011-06-27 14:28:01+00:00</td>\n",
       "      <td>And the glazier who fixed all the broken windo...</td>\n",
       "      <td>2701243</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5811403</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>1370234048</td>\n",
       "      <td>2013-06-03 04:34:08+00:00</td>\n",
       "      <td>Does canada have the equivalent of H1B/Green c...</td>\n",
       "      <td>5804452</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21623</td>\n",
       "      <td>AF</td>\n",
       "      <td>AF</td>\n",
       "      <td>1178992400</td>\n",
       "      <td>2007-05-12 17:53:20+00:00</td>\n",
       "      <td>Speaking of Rails, there are other options in ...</td>\n",
       "      <td>21611</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10159727</td>\n",
       "      <td>EA</td>\n",
       "      <td>EA</td>\n",
       "      <td>1441206574</td>\n",
       "      <td>2015-09-02 15:09:34+00:00</td>\n",
       "      <td>Humans and large livestock (and maybe even pet...</td>\n",
       "      <td>10159396</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2988424</td>\n",
       "      <td>Iv</td>\n",
       "      <td>Iv</td>\n",
       "      <td>1315853580</td>\n",
       "      <td>2011-09-12 18:53:00+00:00</td>\n",
       "      <td>I must say I reacted in the same way when I re...</td>\n",
       "      <td>2988179</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  by author        time                   time_ts  \\\n",
       "0   2701393  5l     5l  1309184881 2011-06-27 14:28:01+00:00   \n",
       "1   5811403  99     99  1370234048 2013-06-03 04:34:08+00:00   \n",
       "2     21623  AF     AF  1178992400 2007-05-12 17:53:20+00:00   \n",
       "3  10159727  EA     EA  1441206574 2015-09-02 15:09:34+00:00   \n",
       "4   2988424  Iv     Iv  1315853580 2011-09-12 18:53:00+00:00   \n",
       "\n",
       "                                                text    parent deleted  dead  \\\n",
       "0  And the glazier who fixed all the broken windo...   2701243    None  None   \n",
       "1  Does canada have the equivalent of H1B/Green c...   5804452    None  None   \n",
       "2  Speaking of Rails, there are other options in ...     21611    None  None   \n",
       "3  Humans and large livestock (and maybe even pet...  10159396    None  None   \n",
       "4  I must say I reacted in the same way when I re...   2988179    None  None   \n",
       "\n",
       "   ranking  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out Users that have less than 10 comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No addon.  It's new install on a fresh new machine.\n",
      "\n",
      "A more fair comparison: the drafts of the ISO C and ISO C++ standards on my system (both from 2011) are 701 respectively 1325 pages, of which 180 respectively 410 describe the language, and the remainder the support library.That difference isn't as large as I would have expected. Part of that may be style or formatting; C++ seems to have about a third more lines per page and smaller text, so I guesstimate it has about 50% more text per page. Looking at the descriptions of the preprocessor that seems about right; C needs 19 pages to do that, while C++ manages it in 13.That would make the C++ language description about 3 times as long as that of C.\n",
      "\n",
      "Cool. I don't have enough to support.\n",
      "\n",
      "Disabled JavaScript?\n",
      "\n",
      "They're called turtles because, back in the day, they were actual robots that looked like turtles. http://www.ias.uwe.ac.uk/Robots/gwonline/gwonline.html\n",
      "\n",
      "What legal remedy do you feel should be available to Samsung in this case?Again, monetary damages. Sufficient for Samsung to publicize their point of view.Why isn't it fair that they be required to publish a retraction?The loser in a case like this may not necessarily agree with the judge's conclusion. The judge forcing them to say something, is tantamount to forcing them to tell a lie.Bear in mind that the UK does not have anything resembling the First Amendment in the US Constitution, and even in the US, libel in particular and false advertising in general are not considered eligible for \"free speech\" defenses.I'm not saying that Apple has a free speech right to make false claims. I'm saying that they have a free speech right to not be forced to make claims by others that they may not agree with. That's a novel way of looking at free speech.People should not be forced to say things. That's wrong. Regardless of jurisdiction.\n",
      "\n",
      "> Your second paragraph about accomplishments.What I imagine is troubling to most is your claim to entrepreneurship skill, but your lack of anything derived from it. 2009 and 2010 were both losing years for you. June 2010 you only had 9k in the bank (related post about your goals for the next few years).Especially since this community is based squarely on entrepreneurship, I think \"money talks, bullshit walks\" is especially relevent to the typical HN user.Also, probably not the same for most people, but I do not understand much about your first paragraph filled with many extra details that don't excite me (full ride to umass, 65k this year, paid cash for everything in Boston, etc). I dont think that explains who you are (well, some might say braggart) or what you're about.I have never met you, but I know many users here have and every single one vouches for your credibility. That is something I believe in.\n",
      "\n",
      "Why wouldn't you be respected equally? The point of a degree is that it's a proxy for a statement about skills, not that the prestige of the degree itself is valuable.\n",
      "\n",
      "I've always found this fish-is-holier-than-red-meat take a bit confusing.  Overfishing is a huge problem worldwide.  Eating responsibly farmed beef, while expensive, seems less impactful than eating many (most?) types of commonly consumed fish.\n",
      "\n",
      "Just watch the ATAP keynote on Google IO 2015 web site.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import html\n",
    "\n",
    "\n",
    "def cleanup_html(raw_html):\n",
    "    \"\"\"\n",
    "    Clean's up raw HTML code to proper format\n",
    "    \"\"\"\n",
    "    clean_html = re.sub(r'<.*?>', '', raw_html)\n",
    "    clean_html_http = re.sub(r'http\\S+([\\.]{3})?', '', clean_html)\n",
    "    clean_txt = html.unescape(clean_html)\n",
    "    return clean_txt\n",
    "\n",
    "# Apply the function\n",
    "comm_df['text'] = comm_df['text'].apply(cleanup_html)\n",
    "\n",
    "# Check results\n",
    "comm_df.sample(10)\n",
    "\n",
    "for row in comm_df['text'].sample(10):\n",
    "    print(row)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing ML on our dataframe\n",
    "\n",
    "\n",
    "### Vader Sentimental Analysis\n",
    "\n",
    "According to Urban Dictionary, a salty person is someone that’s bitter (kinda weird since bitter and salty are completely different tastes, but the transformation of the English language is a topic for another day). Can we predict which users of Hacker News are the saltiest/most toxic based on the comments they post? Can we help users identify whose comments on Hacker News to ignore in order to make their time on the site more enjoyable? How will we determine what “salty” means?\n",
    "\n",
    "For this sentiment analysis model, we will use Vader Sentiment due to its simplicity and ability to handle text typically found on social media (robust measures regarding slang, capitalized letters, emojis, and punctuation). In order to determine a user's \"saltiness\", we will utilize 3 of Vader's polarity scores: positivity, compound, and negative. Positive and negative scores are self-explanatory; however, the compound score is worth understanding further. According to the Vader Sentiment documentation:\n",
    "\n",
    "    The compound score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate.\n",
    "\n",
    "Furthermore, the documentation breaks down how sentiment is obtained:\n",
    "\n",
    "    Typical threshold values (used in the literature cited on this page) are:\n",
    "\n",
    "    positive sentiment: compound score >= 0.05\n",
    "    neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "    negative sentiment: compound score <= -0.05\n",
    "\n",
    "    The pos, neu, and neg scores are ratios for proportions of text that fall in each category (so these should all add up to be 1... or close to it with float operation). These are the most useful metrics if you want multidimensional measures of sentiment for a given sentence.\n",
    "\n",
    "With this understanding, we can now derive a formula to determine the saltiness of our users' comments. For our purposes, we want to give a bit more weight to the positive and negative ratios, so we will define our score formula as follows:\n",
    "\n",
    "    **Saltiness Score** = *Positive Ratio* + *Compound Score* - *Negative Ratio*\n",
    "\n",
    "\n",
    "We only need to perform sentiment analysis on the users' comments, so we'll only focus on the 'text' column. The goal here is to perform an analysis on each comment, and append the comment's score to a corresponding 'score' column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sentiment analysis function\n",
    "\n",
    "def sentiment_score(comment):\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    x = 0\n",
    "    score = analyser.polarity_scores(comment)\n",
    "    x = x + score['pos']\n",
    "    x = x + score['compound']\n",
    "    x = x - score['neg'] \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-dc337e21a0f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Apply the function to each sample in the 'text' column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Store score in newly-created 'score' column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcomm_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'salty_score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomm_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/salty-hacker-ds/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4043\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4044\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4045\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4047\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-b8195f89c373>\u001b[0m in \u001b[0;36msentiment_score\u001b[0;34m(comment)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msentiment_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0manalyser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/salty-hacker-ds/lib/python3.7/site-packages/vaderSentiment/vaderSentiment.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lexicon_file, emoji_lexicon)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexicon_full_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicon_full_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_lex_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0memoji_full_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_this_module_file_path_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memoji_lexicon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/salty-hacker-ds/lib/python3.7/site-packages/vaderSentiment/vaderSentiment.py\u001b[0m in \u001b[0;36mmake_lex_dict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mlex_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicon_full_filepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m             \u001b[0mlex_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlex_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Apply the function to each sample in the 'text' column\n",
    "# Store score in newly-created 'score' column\n",
    "comm_df['salty_score'] = comm_df['text'].apply(sentiment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results\n",
    "comm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pushing the dataframe to postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all we have left to do is convert our pandas dataframe to SQL and load it into our postgres database. For this project, we chose to employ the help of ElephantSQL for its simple interface and exceptional DBMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection to database\n",
    "engine = create_engine('postgres://txtqhcho:mHEV5Or0MiRw_5oaIJF162BkmqapzanU@salt.db.elephantsql.com:5432/txtqhcho')\n",
    "# Covert dataframe to SQL\n",
    "comm_df.to_sql('saltycomments', con=engine, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Salty-Hacker-App (Python 3.7)",
   "language": "python",
   "name": "salty-hacker-app"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
